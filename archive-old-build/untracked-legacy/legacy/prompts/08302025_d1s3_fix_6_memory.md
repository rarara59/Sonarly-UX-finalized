here’s the “Toyota” fix for the memory leak: a tiny, surgical change-set that makes every request clean up after itself, prunes stuck/expired work, and shuts down timers/listeners reliably. No fancy math, just guaranteed cleanup.

What’s (likely) leaking
	•	Requests that timeout/fail leave behind AbortControllers, timers, and inFlight/queue entries.
	•	Expired queue items aren’t always removed or their deferreds nulled.
	•	Background intervals/listeners aren’t torn down, so references keep objects alive.

Minimal, working fix (drop-in)

Apply these three changes in your RpcConnectionPool (≈25 lines total). They’re intentionally boring.

1) Always-cleanup per request (finally)

Make sure every call clears its timeout, aborts its controller, and removes its in-flight record even on errors/aborts.

// inside RpcConnectionPool.call(method, params=[], opts={})
const controller = new AbortController();
const reqId = Symbol(method);
const startedAt = Date.now();
const timeoutMs = this.config.requestTimeoutMs ?? 5000;
const timer = setTimeout(() => controller.abort(), timeoutMs);

// track in-flight so we can prune if anything goes wrong
this.inFlight ??= new Map();
this.inFlight.set(reqId, { controller, startedAt });

try {
  return await this._callOnce(method, params, { ...opts, signal: controller.signal });
} finally {
  clearTimeout(timer);
  controller.abort();           // safe if already settled
  this.inFlight.delete(reqId);  // drop reference
  // break any accidental closures
  // (helps V8 free memory quicker under pressure)
}

2) Prune expired queue items (and null deferreds)

Before dispatching, drop anything past queueDeadline and always reject & null out the deferred to break references.

// at top of processQueue()
const now = Date.now();
const deadline = this.config.queueDeadline ?? 60_000;
for (let i = 0; i < this.requestQueue.length; ) {
  const r = this.requestQueue[i];
  if (now - (r.timestamp ?? now) > deadline) {
    // reject once, then sever links so nothing is retained
    try { r.deferred?.reject?.(new Error('request expired')); } catch {}
    r.deferred = null;
    r.params = null;
    this.requestQueue.splice(i, 1);
    continue;
  }
  i++;
}

3) Add a tiny “leak guard” and clean shutdown

A one-minute sweeper that aborts anything stuck beyond 2× timeout, and a destroy that clears intervals/listeners. Use .unref() so it doesn’t keep the process alive.

// start once (e.g., in constructor)
if (!this._leakGuard) {
  this._leakGuard = setInterval(() => {
    const now = Date.now();
    const maxAge = 2 * (this.config.requestTimeoutMs ?? 5000);
    for (const [id, rec] of this.inFlight ?? []) {
      if (now - (rec.startedAt ?? now) > maxAge) {
        rec.controller?.abort();
        this.inFlight.delete(id);
      }
    }
  }, 60_000);
  this._leakGuard.unref?.();
}

// in destroy()
if (this._leakGuard) clearInterval(this._leakGuard);
this._leakGuard = null;
for (const { controller } of this.inFlight ?? []) controller.abort();
this.inFlight?.clear?.();
this.removeAllListeners?.(); // if EventEmitter

Why this works (and keeps <2%/hour growth)
	•	Guaranteed completion path: finally runs on success, error, and timeout → no stray timers/controllers/maps.
	•	Queue hygiene: Expired items are removed, their deferreds/params nulled → no “dangling promises” retaining memory.
	•	Backstop: The leak guard sweeps rare edge cases (bugs, library hangs, partial failures) without adding complexity.
	•	Clean teardown: Intervals/listeners are cleared so long-running processes don’t accumulate references.

How to validate quickly (5–10 min)
	1.	Run your sustained load test (20 concurrent) with memory logs:

node --expose-gc scripts/test-sustained-load.js
# or add a tiny logger in the test:
setInterval(() => {
  global.gc?.();
  const m = process.memoryUsage();
  console.log(`[mem] heapUsedMB=${(m.heapUsed/1024/1024).toFixed(1)}`);
}, 60_000).unref();

	2.	Watch the slope for minutes 2→10. You want a flat(ish) line. Extrapolated growth <2%/hour is the target.
	3.	Sanity check:
	•	Kill the network (simulate timeouts) → memory should not creep.
	•	Flood then idle → memory returns near baseline (no retained queue items).

Notes / gotchas
	•	Don’t add extra state; just remove it. The point is fewer moving parts.
	•	Avoid capturing large objects in closures you store on the queue. Keep entries small ({method, params, deferred, timestamp}) and null fields after resolve/reject.
	•	If you emit per-request events (e.g., 'request-complete'), prefer once() or remove listeners explicitly.

Expected impact on your metrics
	•	Memory growth: from ~339%/hr → typically flat or <2%/hr under sustained load.
	•	Success rate / latency: unchanged (this is cleanup only).
	•	Operational risk: lower — fewer latent references, fewer “mystery” handles.